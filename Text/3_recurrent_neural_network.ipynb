{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wikistat/AI-Frameworks/blob/master/Text/3_recurrent_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IA Frameworks](https://github.com/wikistat/AI-Frameworks) - Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data : Cdiscount's product description.\n",
    "\n",
    "This dataset has been released from Cdiscount for a data competition (type kaggle) on the french website [datascience.net](https://www.datascience.net/fr/challenge). <br>\n",
    "The test dataset of this competition has not been released, so we used a subset of 1M of products of the original train dataset(+15M rows) all along the **Natural Language Processing** lab.<br>\n",
    "The objective of this competition was to classify the text description of various product into various categories that compose the navigation tree of Cdiscount website. It is composed of 4,733 categories organized within 44 meta categories. <br>\n",
    "\n",
    "The objective of this lab is not win the competition so we will only used the meta-categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files & Data (Google Colab)\n",
    "\n",
    "If you're running this notebook on Google colab, you do not have access to the `data` or `solutions` folder you get by cloning the repository locally. \n",
    "\n",
    "The following lines will allow you to build the folders and the files you need for this TP.\n",
    "\n",
    "**WARNING 1** Do not run this line localy.\n",
    "**WARNING 2** The magic command `%load` does not work work on google colab, you will have to copy-paste the solution on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! mkdir data\n",
    "! wget -P data https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/cdiscount_test.csv.zip\n",
    "! wget -P data https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/cdiscount_train.csv.zip\n",
    "! wget -P data https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/description_coque.npy.zip\n",
    "! unzip data/description_coque.npy.zip -d data/\n",
    "! wget -P data https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/char_to_int.pkl\n",
    "! wget -P data https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/int_to_char.pkl\n",
    "! wget -P data https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/generate_model.h5\n",
    "! mkdir solution\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/many_to_one_toy_example.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/simple_rnn_output.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/many_to_many_toy_example.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/simple_rnn_output_bis.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/one_to_many_dataset.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/one_to_many_model.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/one_to_many_prediction.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/encode_input_output_sequence.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/train_model_text_generation.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/text_generation_random_first_letter.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/text_generation_multinomial.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/token_to_embedding_sequence\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/rnn_classifier_model.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/clean.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Recurrent Neural Network.\n",
    "\n",
    "The objectives of these first notebook are the following:\n",
    "\n",
    "* Build various *RNN* architecture with `keras` on toy example. ( **Many to one**, **one to many**, **many to many**, **bidirectional layer**, **Deep NN**, etc.\n",
    "* Use *RNN* for **Text Generation**. Generate product description.\n",
    "* Use *RNN* for **Text classification**. As the two precedent notebook, we will use Recurrent Neural Network algorithm to predict product's category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sklearn.model_selection as sms\n",
    "from solution.clean import CleanText\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tutorial\n",
    "\n",
    "This part aims to understand how to build the different types of RNN models (**one/many-to-one/many**) with `keras`.\n",
    "\n",
    "This part is strictly pedagogical and toy data will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to one\n",
    "\n",
    "**Many-to-one** recurrent neural network takes a sequence as an input and return a scalar as an output :\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/many_to_one.png?raw=true\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example\n",
    "\n",
    "An **RNN**  model always take 3 dimensional tensors as  inputs AND outputs which are (in this order when using `Keras`):\n",
    "\n",
    "* The **Batch**. (Number of sequences in the dataset) \n",
    "* The **Timestep**. (The size of one sequence)\n",
    "* The **Features**. (How many features for each element of each sequence)\n",
    "\n",
    "Let's take an example where:\n",
    "\n",
    "* the *input* are sequences of 3 numbers \n",
    "* the *output* the sum of these 3 numbers.\n",
    "* The dataset contains 100 individuals.\n",
    "\n",
    "**Exercise**: What would be the dimension of the input `X` and output `Y` matrix ?\n",
    "Fill the cell below with correct dimensions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(??).reshape(?,?,?)\n",
    "print(\"Dimensions of input sequences: %d, Timestep: %d, Number of features: %d\" %X.shape)\n",
    "Y = X.sum(1).reshape(100,?,?)\n",
    "print(\"Dimensions of output sequences: %d, Timestep: %d, Number of features: %d\" %Y.shape)\n",
    "print(\"Input Example\")\n",
    "print(X[0])\n",
    "print(\"Output Example\")\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/many_to_one_toy_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The following lines enable to define a very simple model with one **many-to-one** *RNN* layer with:\n",
    "\n",
    "* 10 neurons (*units=10)\n",
    "* a *relu* activation layer\n",
    "\n",
    "This model take as an input sequences of size (3,1). Note that the *input_shape* argument does not take the batch size as an input. Only the *timestep* and the *feature sie*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(3, 1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Do the shape of the output seems normal to you? What do the two dimensions represent? <br>\n",
    "**Exercise** : Send a sequence trough the model and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/simple_rnn_output.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the complete model. <br>\n",
    "The output of the RNN layer are vectors of size 10 for each individual. <br>.\n",
    "Let's add a Dense layer so that the final output is of size 1 for each individual, *i.e.* the dimension we want as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(3, 1)))\n",
    "model.add(kl.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model with and *adam* optimizer and a *mse* as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size=32\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model can now correctly perform the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array([10,11,12]).reshape(1,3,1)\n",
    "print(model.predict(x_test))\n",
    "x_test=np.array([10,25,12]).reshape(1,3,1)\n",
    "print(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if  input_shape = (1,1) we set a **one-to-one** recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With no fix timestep\n",
    "\n",
    "Note that in the previous example, the timestep was fix to three. But it's possible to set the parameters to *None* so that the model can handle sequences of variable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(None, 1)))\n",
    "model.add(kl.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if this model can handle sequence of variable **timestep** lengths, during training, all sequences should have the same **timestep** lengths.\n",
    "\n",
    "To handle this, we apply zero padding to the sequences of variables length so that it does not affect the results thanks to the `pad_sequences` function from `keras`.\n",
    "\n",
    "Let's first create a X list of sequences of different **timestep** size (3 or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for x in np.arange(0,300,3):\n",
    "    X.append([x,x+1,x+2])\n",
    "for x in np.arange(300,500,2):\n",
    "    X.append([x,x+1])\n",
    "print(\"Number of sequences by timestep length\")\n",
    "collections.Counter([len(x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now pad this sequence with zero values. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, value=0.0, padding = 'pre')\n",
    "print(\"X shape : (%d,%d)\" %X.shape)\n",
    "print(\"3 first sequences\")\n",
    "print(X[:2])\n",
    "print(\"2 last sequences\")\n",
    "X[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks about padding\n",
    "\n",
    "* Note that a good practices aims to pad value to the **left** of the sequences. \n",
    "* This can be not intuitive but the reason is that nothing is learn at the beginning of the sequence because all the values would be zeros, the real learning would start when first non zeros values appears. \n",
    "* If the sequences are padded to the right, the information learn on the beginning of the sequences could be lost passing through all zeros values at the end of the sequences.\n",
    "* Padding values depends of the objective. Here sequences are padded with zero value so that it doesn't change the values of the sum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(200,3,1)\n",
    "Y = X.sum(1)\n",
    "\n",
    "epochs = 500\n",
    "batch_size=32\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict sum of sequences of different lenght (even bigger sequences, but results is not guaranted!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array([3,4]).reshape(1,2,1)\n",
    "print(model.predict(x_test))\n",
    "x_test=np.array([3,4,5]).reshape(1,3,1)\n",
    "print(model.predict(x_test))\n",
    "x_test=np.array([3,4,5,6]).reshape(1,4,1)\n",
    "print(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to Many\n",
    "\n",
    "**Many-to-many** recurrent neural network take a sequence as an input and return a sequence as an output :\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/many_to_many.png?raw=true\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Example\n",
    "\n",
    "Let's take an example where the *input* are sequences of 3 number  and the output will be a sequences of cumulative sum, i.e.\n",
    "\n",
    "* input = [x1, x2, x3]\n",
    "* output = [x1, x1+x2, x1+x2+x3]\n",
    "\n",
    "**Exercise**: What would be the dimension of the input `X` and output `Y` matrix ?\n",
    "Fill the cell below with correct dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(??).reshape(100,??,??)\n",
    "print(\"Dimensions of input sequences: %d, Timestep: %d, Number of features: %d\" %X.shape)\n",
    "Y = X.cumsum(1).reshape(100,??,??)\n",
    "print(\"Dimensions of input sequences: %d, Timestep: %d, Number of features: %d\" %Y.shape)\n",
    "print(\"Input Example\")\n",
    "print(X[0])\n",
    "print(\"Output Example\")\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/many_to_many_toy_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "The following lines enable to define a very simple model with one **many-tom-many** *RNN* layer with:\n",
    "\n",
    "* 10 neurons (*units=10)\n",
    "* a *relu* activation layer\n",
    "\n",
    "This model take as an input sequences of size (3,1) and return a sequence of the same size.<br>\n",
    "This is specified but the `return_sequences` argument which is set to True. (and set to False by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(3, 1), return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Do the shape of the output seems normal to you? What do the three dimensions represent? <br>\n",
    "**Exercise** : Send a sequence trough the model and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/simple_rnn_output_bis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the complete model. <br>\n",
    "For each input sequences, the output of the RNN layer is a matrix of size 3 (number of **timestep**) per 10  (**features**). <br>\n",
    "The desired output would be a sequence of size 3  per 1.\n",
    "\n",
    "In order to obtain the correct dimension let's add a Dense layer at **each timestep** to get the desired output. <br>\n",
    "This can be done with the `TimeDistributed` layer of `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(3, 1), return_sequences=True))\n",
    "model.add(kl.TimeDistributed(kl.Dense(1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model with and *adam* optimizer and a *mse* as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size=32\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model can now correctly perform the cumulative sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array([10,11,12]).reshape(1,3,1)\n",
    "print(model.predict(x_test))\n",
    "x_test=np.array([10,25,12]).reshape(1,3,1)\n",
    "print(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as previously seen, it would have been possible to set the *timestep* parameters to *None* so that the model can compute cumulative sum of model whatever the size of their length (using padding).  \n",
    "This could be a good **exercise** if you want to practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One to Many\n",
    "\n",
    "**One-to-many** recurrent neural network take a scalar as an input and return a sequence as an output. \n",
    "\n",
    "There are different ways to define **one-to-many**\n",
    "neural network. \n",
    "\n",
    "* In the example below, the **one-to-many** network can be seen as as a **many-to-many** neural network where the input sequence is build iteratively. (The input of the second timestep is the output of the first timestep).\n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/one_to_many.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "* It would also be possible to only pass an input at the first timestep. Then **one-to-many** network can be seen as as a **many-to-many** neural network where the input sequence is composed of one scalar and `None` entry to fill the sequences. \n",
    "\n",
    "Hence, **one-to-many** networks, can be seen as particular case of **many-to-many** neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Example\n",
    "\n",
    "Let's take an example where the *input* are scalar and the output is sequence of 3 number composed such that\n",
    "\n",
    "* input = x\n",
    "* output = [x+2, x+4, x+6]\n",
    "\n",
    "Hence the dimensions of the output matrix *Y* will be of size:\n",
    "\n",
    "* N: Number of sequences = 100 (arbitrary values), \n",
    "* Timestep: (Size of sequences) = 3, \n",
    "* Number of features (How many features for each element of the sequences) = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "At **training**  the keras model will be built as a **many-to-many** models. <br>\n",
    "Indeed as you now the sequence output you're expect to get, you now the sequence that will be send as an input you want to learn. IN the example above:\n",
    "\n",
    "* input = [x,x+2,x+4].\n",
    "* output = [x+2,x+4,x+6]\n",
    "\n",
    "**Exercise**: Build the toy dataset and the models that will learn how to predict the output sequences from and input sequences.<br>\n",
    "**nb** Remember that at prediction, the model should be able to take a scalar as an input (i.e. a sequence of one timestep).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/one_to_many_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/one_to_many_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size=32\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Once your model is build, write a function that build a the 3 numbers sequences output from a scalar input using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/one_to_many_prediction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the examples details so far have treated *one-size* features sequences in order to make this tutorial easier. \n",
    "\n",
    "All of these examples can be easily traduced to *several-size* features length. Let's check that with example on the **Cdisocunt** Dataset! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN layers\n",
    "\n",
    "Once you know how to manipulate the structure defined above, it is really easy to build more complex or deepest RNN model with keras.\n",
    "\n",
    "* `GRU` and `LSTM` can be used the exact same way than `SimpleRNN`in the example above.\n",
    "* Bi-directional layers can be build using the `Bidirectional` layer on `RNN`layer.\n",
    "* Deep RNN can be build adding `RNN` layer like any other sequential model.\n",
    "\n",
    "***Example***:\n",
    "Here is how to build a model with one *LSTM* layer follow by a bidirectional *GRU* layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.LSTM(units=10 ,activation=\"relu\", input_shape=(3, 1), return_sequences=True))\n",
    "model.add(kl.Bidirectional(kl.GRU(units=10 ,activation=\"relu\", return_sequences=True)))\n",
    "model.add(kl.TimeDistributed(kl.Dense(1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "Now, you know how to build all combination of **One/Many to One/many** RNN models using all `Keras` tools:\n",
    "* `return_sequences` RNN layer's parameter : return one output or a sequence output. \n",
    "*  `input_shape` parameter  : with None value has the timestep if we want the model to handle various length sequences.\n",
    "* `TimeDistributed` layer: in order to apply layer such as `Dense` on each entries of a sequence.\n",
    "* `Gru`and `LSTM` as alternative layer of `SimpleRNN`.\n",
    "* `Bidirectional layer`.\n",
    "\n",
    "Let us use this knowledge to handle real use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation \n",
    "\n",
    "In this part the objective is to generate product description of a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The Level 3 category `COQUE - BUMPER - FACADE TELEPHONE` is the most represented category within the original **Cdiscount**'s dataset with 2.184.671 descriptions. Among them 1.761.637 are composed with 197 characters. \n",
    "\n",
    "We will now use these lines (or sub-samble of these lines according to the computation power of your machine) in order to learn a text generation model that will allow to automatically generate a new text description of this type of product.\n",
    "\n",
    "These lines are contains in the `data/description_coque.npy` file (you have to unzip it).\n",
    "\n",
    "The data used in this section are real data. However it turns out that a lot of these description are very similar. This help the network to build good description but which are not really different from the training set. <br>\n",
    "However, this is a good thing to obtains result in a reasonable amount of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "X = np.load(\"data/description_coque.npy\")[:N]\n",
    "print(\"Number of line :%d\" %X.shape[0])\n",
    "print(\"\\nThree lines example:\")\n",
    "print(X[:3])\n",
    "print(\"\\nSize of all the sequences : %s\" %(str(set([len(x) for x in X]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "The text generation implies to build a **one-To_many** model:\n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/one_to_many.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Where the prediction $y_t$ will be used as an input at time $t+1$, i.e : $y_t=x_{t+1}$. \n",
    "\n",
    "This model with be trained as a **many-to-many**  model. \n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/many_to_many.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Where the output y will be the same sequence than input x with 1 offset.\n",
    "\n",
    "When using text, the input can be either a word or a characters. As sequences have fixed length, we will use the characters as inputs of the sequences. <br>\n",
    "These characters will be *one-hot encoded* <br>\n",
    "\n",
    "Hence each description $x$ will be represented as a Matrix of size $N_s \\times N_v$ where\n",
    "\n",
    "* $N_s=197$ is the length of the sequences (timestep)\n",
    "* $N_v$ is the size of the vocabulary (the list of caracters) .\n",
    "\n",
    "Let us fix $N_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns=197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters' list\n",
    "\n",
    "Let's first create a list of all unique characters present in the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_set = list(functools.reduce(lambda x,y : x.union(y), [set(x) for x in X], set()))\n",
    "print(\"Characters list of size %d : %s\"  %(len(chars_set), str(chars_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add two elements to these listes allowing to detect the *start* and the *end* of a sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_set.extend([\"START\",\"END\"])\n",
    "Nv = len(chars_set)\n",
    "print(\"Total size of the vocabulary : %d\" %Nv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence encoding\n",
    "\n",
    "There are no library (or I do not find it), that enable to *one-hot encode* a string at a character level.\n",
    "\n",
    "The following lines enables to apply it.\n",
    "\n",
    "* First `char_to_int` and `int_to_char` dictionary are created, enabling to retrieve the position of a character in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = {i:c for i,c in enumerate(chars_set)}\n",
    "char_to_int = {c:i for i,c in int_to_char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function encode\n",
    "\n",
    "* a  $X\\in \\mathbb{R}^{N \\times N_s}$ matrix composed of *N* text description of size *N_s*   size\n",
    "\n",
    "into\n",
    "\n",
    "* a $X_{vec} \\in \\mathbb{R}^{N \\times N_s +1 \\times N_v}$ matrix composed of *N* sequences of size $N_s+1\\times N_v$ (the encoded text description)\n",
    "and \n",
    "* a $Y_{vec} \\in \\mathbb{R}^{N \\times N_s +1 \\times N_v}$ matrix composed of *N* sequences of size $N_s+1\\times N_v$ (the encoded text description with an offset of one).\n",
    "\n",
    "Note that the length of the sequences in $X_{vec}$ and $Y_{vec}$ are of size $N_s+1$  because we will add an element to each of these sequences\n",
    "\n",
    "* Input sequences will have a *START* element at their beginning.\n",
    "* Output sequences will have a *END* element at their end.\n",
    "\n",
    "\n",
    "**exercise** Let us try to encoded the function that apply these transformation. -> Read the test cell below before to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_output_sequence(x_descriptions, length_sequence, size_vocab, char_to_int_dic):\n",
    "    return x_vec, y_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test cell**\n",
    "\n",
    "To help you coding this function, here is some test to ensure that you function has the expected behavior:\n",
    "\n",
    "If the function take a single sentence of length 7 (*bonjour*) as en input:\n",
    "\n",
    "* Both X (input of the rnn) and Y (output of the rnn) outputs should be tensor of shape (1 X 8 X 91).\n",
    "* For X, by taking the argmax of each element of the sequence, we should be able to retrieve the original sentence (with START element at the beginning)\n",
    "* For Y, by taking the argmax of each element of the sequence, we should be able to retrieve the original sentence (with END element at the END)\n",
    "\n",
    "**Warning** you do not have to modify this cell! All test have to work if you function is correctly implemented. <br>\n",
    "If a test fail it will throw a `Assertion Error`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,Y_test = encode_input_output_sequence(np.array([\"bonjour\"]), 7, Nv, char_to_int)\n",
    "assert X_test.shape == (1, 8, Nv)\n",
    "assert Y_test.shape == (1, 8, Nv)\n",
    "\n",
    "assert [int_to_char[x] for x in np.argmax(X_test[0],axis=1)] == ['START', 'b', 'o', 'n', 'j', 'o', 'u', 'r']\n",
    "assert [int_to_char[x] for x in np.argmax(Y_test[0],axis=1)] == ['b', 'o', 'n', 'j', 'o', 'u', 'r', 'END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/encode_input_output_sequence.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it on the first N lines of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec, Y_vec = encode_input_output_sequence(X[:N], Ns, Nv, char_to_int)\n",
    "X_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "**Exercise**: Define a simple model (only one LSTM layer with 32 hidden units) that will allow to train the text generation model.\n",
    "*Tips*:\n",
    "* Remember that this model will be used for generation.\n",
    "* What are the dimension of the output? What will be the activation layer? The loss function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/train_model_text_generation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have correctly write the model you can observe that it can take a while to obtain convergence when training these kind of model. <br>\n",
    "If you do not have GPU while executing this TP, you do not have to wait for the end of the training. -> Let's download this model, generated with the solution above! <br>\n",
    "Note that you also have to download the corresponding `int_to_char`and `char_to_int` dictionary.\n",
    "And re build the dataset according to these dictionaries;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"data/generate_model.h5\")\n",
    "int_to_char = pickle.load(open(\"data/int_to_char.pkl\",\"rb\"))\n",
    "char_to_int = pickle.load(open(\"data/char_to_int.pkl\",\"rb\"))\n",
    "X_vec, Y_vec = encode_input_output_sequence(X[:N], Ns, Nv, char_to_int)\n",
    "X_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below, enable to decode en encoded sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_test = 50\n",
    "print(\"\\nOriginal Sentences:\\n%s\"%X[i_test])\n",
    "def decode_sequence(x, int_to_char_dic):\n",
    "    seq = []\n",
    "    for i in np.where(x)[1]:\n",
    "        seq.append(int_to_char_dic[i])\n",
    "    return \"\".join(seq)\n",
    "print(\"\\nDecoded input vector::\\n%s\"%decode_sequence(X_vec[i_test], int_to_char))\n",
    "print(\"\\nDecoded output vector::\\n%s\"%decode_sequence(Y_vec[i_test], int_to_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below enable to predict a sentence by iterativaly predict a character sending a previous character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import convert_to_tensor\n",
    "x_pred = np.zeros((1, Ns+1, Nv))\n",
    "print(\"step 0\")\n",
    "x_pred[0,0,char_to_int[\"START\"]] =1\n",
    "x_pred_str = decode_sequence(x_pred[0], int_to_char)\n",
    "print(x_pred_str)\n",
    "\n",
    "for i in range(Ns):\n",
    "    x_tensor = convert_to_tensor(x_pred[:,:i+1,:])\n",
    "    ix = np.argmax(model.predict(x_tensor)[0][-1,:])\n",
    "    x_pred[0,i+1,ix] = 1\n",
    "x_pred_str=decode_sequence(x_pred[0], int_to_char)\n",
    "print(x_pred_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** How this prediction is done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Generate a text generation with random first letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/text_generation_random_first_letter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Generate a text generation with some randomness. For example, use a multinomial from the model output to generate a characters at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/text_generation_multinomial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "In this part the objective is to classify description of `Cdiscount` product as in the two first notebooks of this **Text** session.\n",
    "\n",
    "We will the use a **many-to-one** RNN architecture both at training and prediction.\n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/many_to_one.png?raw=true\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Let us first load and clean the data with the function you now know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "data = pd.read_csv(\"data/cdiscount_train.csv.zip\",sep=\",\", nrows=100000)\n",
    "ct.clean_df_column(data, \"Description\", \"Description_cleaned\")\n",
    "print(\"The train dataset is composed of %d lines\" %data.shape[0])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"data/cdiscount_test.csv.zip\",sep=\",\")\n",
    "ct.clean_df_column(data_test, \"Description\", \"Description_cleaned\")\n",
    "print(\"The train dataset is composed of %d lines\" %data_test.shape[0])\n",
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split the data into train and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = sms.train_test_split(data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get list of words for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array_token = [line.split(\" \") for line in data_train[\"Description_cleaned\"].values]\n",
    "valid_array_token = [line.split(\" \") for line in data_valid[\"Description_cleaned\"].values]\n",
    "test_array_token = [line.split(\" \") for line in data_test[\"Description_cleaned\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And finally list of array of integer label (keras does not handle string label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_int = {k:i for i,k in enumerate(set(data_train.Categorie1.values))}\n",
    "N_label = len(label_to_int)\n",
    "Y_train = np.array([label_to_int[k] for k  in data_train.Categorie1.values])\n",
    "Y_valid = np.array([label_to_int[k] for k in data_valid.Categorie1.values])\n",
    "Y_test = np.array([label_to_int[k] for k in data_test.Categorie1.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the data.\n",
    "\n",
    "In this problem sequences of word will be considered (and not sequence of characters). <br>\n",
    "Several possibilities are possible to convert the sentences:\n",
    "\n",
    "* Count Vectorizer or TF-IDF encoding. \n",
    "    * **sklearn** `CountVectorizer` or  `TF-IDF` produce sparse vector that are not handling correctly with `Keras` model.<br>    Hence the solutions to use these encoding would be to create non-sparse matrix. But this implies to create really large matrix (regarding to the size of th vocabulary\n",
    "* Word Embedding vectors.\n",
    "    * With the **Word2Vec** embedding we learn in the previous notebook we can create vector of size 300 from each word. which would produce reasonable sequence size. Moreoever, the *full_model_sg* was the model who gives the best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_sg_full = KeyedVectors.load(\"data/w2v_model/full_model_sg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below display the distribution of the length of all the sequences of the train dataset (i.e the number of word).<br>\n",
    "The biggest sequence is composed of 43 words which means that all encoding sequences should have a size of 43   <br>\n",
    "We can see that the 99% percentile of this distribution is 28.\n",
    "Hence we decide to get only the 28 first words of each distribution. and only a very small portion (1%) of the dataset will have cut sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences_length = [len(x) for x in train_array_token]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist(all_sequences_length, 50, cumulative=True, density=True)\n",
    "\n",
    "print(\"Max length sequences: %d\" %max(all_sequences_length))\n",
    "print(\"Percentiles at 90%%: %d, 95%%: %d and 99%%: %d\" %tuple([x for x in np.percentile(all_sequences_length, q=[90,95,99])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Build a function that encode an the `array_token` to tensor that will be used for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_embedding_sequences(array_token, model):\n",
    "    # Todo\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/token_to_embedding_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokens_to_embedding_sequences(train_array_token, model_sg_full)\n",
    "X_valid = tokens_to_embedding_sequences(valid_array_token, model_sg_full)\n",
    "X_test = tokens_to_embedding_sequences(test_array_token, model_sg_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape == (90000,Ns,300) \n",
    "assert X_valid.shape == (10000,Ns,300) \n",
    "assert X_test.shape == (50000,Ns,300) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Build now a model to learn how to predict the classification from this sequence and train it! <br>\n",
    "The model given as a solution is very simple and has not been studied to produce good results. It's just a working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/rnn_classifier_model.py"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
