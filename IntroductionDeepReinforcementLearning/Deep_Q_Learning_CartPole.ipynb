{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wikistat/AI-Frameworks/blob/master/IntroductionDeepReinforcementLearning/Deep_Q_Learning_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IA Frameworks](https://github.com/wikistat/AI-Frameworks) - Introduction to Deep Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1b : Deep Q-Network on CartPole\n",
    "The objectives of this noteboks are the following : \n",
    "\n",
    "* Discover AI Gym environmenth *CartPole* game.\n",
    "* Implement DQN to solve cart pole (a pacman-like game).\n",
    "* Implement Experience Replay Buffer to improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files & Data (Google Colab)\n",
    "\n",
    "If you're running this notebook on Google colab, you do not have access to the `solutions` folder you get by cloning the repository locally. \n",
    "\n",
    "The following lines will allow you to build the folders and the files you need for this TP.\n",
    "\n",
    "**WARNING 1** Do not run this line localy.\n",
    "**WARNING 2** The magic command `%load` does not work work on google colab, you will have to copy-paste the solution on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir solution\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/push_cart_pole.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/DNN_class.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/DQN_cartpole_class.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/play_cartpole_with_dnn.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/DQN_cartpole_memory_replay_class.py\n",
    "\n",
    "! wget -P . https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/experience_replay.py   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import collections\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# To plot figures and animations\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "# Gym Library\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions enable to build a video from a list of images. <br>\n",
    "They will be used to build video of the game you will played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=400):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Gym Librairie\n",
    "<a href=\"https://gym.openai.com/\" ><img src=\"https://gym.openai.com/assets/dist/home/header/home-icon-54c30e2345.svg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<br>\n",
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. <br> It provides many environments for your learning *agents* to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple environment: the Cart-Pole\n",
    "\n",
    "## Description\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "### Observation\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "### Actions\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the righ&t\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Reward\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniform random value between ±0.05\n",
    "\n",
    "### Episode Termination\n",
    "1. Pole Angle is more than ±12°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 200\n",
    "\n",
    "### Solved Requirements\n",
    "Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "\n",
    "The description above if part of the official description of this environemtn. Read full description [here](https://github.com/openai/gym/wiki/CartPole-v0).\n",
    "\n",
    "The following command will load the `CartPole` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reset` command initialize the environement and return the first observation which are a 1D array of size 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "env.observation_space, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What are the four values above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `render` command allows to generate the environment which is here a 400X600 pixels with RGB channels. \n",
    "\n",
    "The `render` command for the `CartPole`environment also open another window that we will close directly with the `env.close`command. It can produce disturbing behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render(mode = \"rgb_array\")\n",
    "env.close()\n",
    "print(\"Environemnt is a %dx%dx%d images\" %img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment can then easily be displayed with matplotlib function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "_ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is composed of two actions push to the left (0), push to the right (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `step function` enables to apply one of this actions and return several information : \n",
    "\n",
    "* The new observation after applying this action\n",
    "* The reward this action has produced\n",
    "* A boolean that indicates if the experience is over or not.\n",
    "* Extra information that depend of the environment (CartPole environment does not provide anything).\n",
    "\n",
    "Let's push the cart pole to the left!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, info = env.step(0)\n",
    "print(\"New observation : %s\" %str(obs))\n",
    "print(\"Reward : %s\" %str(reward))\n",
    "print(\"Is the experience over? : %s\" %str(done))\n",
    "print(\"Extra information : %s\" %str(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render(mode = \"rgb_array\")\n",
    "env.close()\n",
    "plt.imshow(img)\n",
    "axs =  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** : What can you see? Does the output value seems normal to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** : Reset the environment, and push the car to the left untill the experience is over then display the final environment. \n",
    "**Q** : Why do the environment ends? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/push_cart_pole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q network\n",
    "\n",
    "In **Q-learning** all the *Q-Values* are stored in a *Q-table*. \n",
    "The optimal value can be learn by playing game and updating the Q-table with the following formula.\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a')$$\n",
    "$$Q_{k+1}(s,a)\\leftarrow(1-a)Q_k(s,a)+\\alpha[target]$$\n",
    "\n",
    "if the combinations of states and actions are too large, the memory and the computation requirement for the *Q-table* will be too high.\n",
    "\n",
    "Hence, in **Deep Q-learning** we use a function to generate the approximation of the *Q-value* rather than remembering the solutions. <br>\n",
    "As the input of the function, i.e, the *observation*, are vectors of four values, a simple **DNN** will be enough  to approximate the q table\n",
    "\n",
    "Later, we will generate targets from experiences and train this **DNN**.\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a')$$\n",
    "$$\\theta_{k+1} \\leftarrow \\theta_k - \\alpha\\nabla_{\\theta}\\mathbb{E}_{s\\sim'P(s'|s,a)} [(Q_{\\theta}(s,a)-target(s'))^2]_{\\theta=\\theta_k} $$\n",
    "\n",
    "The `DNN` class below defines the architecture of this *neural network*.\n",
    "\n",
    "**Exercise** \n",
    "\n",
    "The architecture of the *dnn* as been set for you<br>\n",
    "\n",
    "However, the shape of the input as well as the number of neurons and the activation function  of the last layer are not filled.<br>\n",
    "Fill the gap so that this network can be use to approximate *Q-values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.lr = 0.001\n",
    "\n",
    "        self.model = km.Sequential()\n",
    "        self.model.add(kl.Dense(150, input_dim=??, activation=\"relu\"))\n",
    "        self.model.add(kl.Dense(120, activation=\"relu\"))\n",
    "        self.model.add(kl.Dense(??, activation=??))\n",
    "        self.model.compile(loss='mse', optimizer=ko.Adam(lr=self.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/DNN_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP Q Learning on *Cartpole*\n",
    "\n",
    "The objective of this section is to implement a **Deep Q-learning** tha will be able to solve the cartpole environment.\n",
    "\n",
    "For that 2 python class will be required:\n",
    "\n",
    "* `DNN`: A class that will enable to use a function that approximate the Q-values\n",
    "* `DQN`: A class that will enable to train the Qnetowrk\n",
    "\n",
    "\n",
    "All the instructions of this section are in this notebook belows. \n",
    "\n",
    "However you will have the possibility to \n",
    "* work with the scripts DQN_cartpole.py and DQN_cartpole_test.py that can be found in the `IntroductionDeepReinforcementLearning`folder\n",
    "* OR work with the codes in cells of this notebok. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Class\n",
    "\n",
    "\n",
    "The `DQN` class contains the implementation of the **Deep Q-Learning** algorithm. The code is incomplete and you will have to fill it!. \n",
    "\n",
    "**GENERAL INSTRUCTION**:\n",
    "\n",
    "* Read the init of the `DQN` class. \n",
    "    * Various variable are set with their definition, make sure you understand all of its.\n",
    "    * The *game environment*, the *memory of the experiences* and the *DNN Q-network* are initialised.\n",
    "* Read the `train` method. It contains the main code corresponding to the **pseudo code** below. YOU DO NOT HAVE TO MODIFY IT! But make sure you understand it.\n",
    "* The `train` method use methods that are not implemented. \n",
    "    * You will have to complete the code of 4 functions. (read instruction of each exercise below)\n",
    "    * After the cell of the `DQN` class code below there are **test cells** for each of these exercices. <br>\n",
    "    This cell should be executed after each exercice. This cell will check that the function you implemented take input and output in the desired format. <br> DO NOT MODIFY this cell. They will work if you're code is good <br> **Warning** The test celle does not guarantee that your code is correct. It just test than input and output are in the good format.\n",
    "\n",
    "\n",
    "#### Pseudo code \n",
    "*We will consider that we reach the expected *goal* if achieve the max score (200 steps without falling)\n",
    "over ten games.*\n",
    "\n",
    "While you didn't reach the expected *goal* reward or the *max_num_episode* allow to be played:\n",
    "* Start a new episode and while the epsiode is not done:\n",
    "    * At each step:\n",
    "        * Run one step of the episode: (**Exercise 1**)\n",
    "        * Save experience in memory: (**Exercise 2 & 3**)\n",
    "        * If we have stored enough episode on the memory to train the batch:\n",
    "            * train model over a batch of targets (**Exercise 4**)\n",
    "            * Decrease probability to play random\n",
    "\n",
    "\n",
    "    \n",
    "**Exercise 1**:  Implement `save_experience`<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; This function save each experience produce by a step on the `memory`of the class.<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; We do not use the experience replay buffer in this part, so you just have to save the last `batch_size`experience in order to use it at the next train step\n",
    "(https://keras.io/api/layers/)\n",
    "    \n",
    "**Exercise 2**:  Implement `choose_action`<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; This method chooses an action in *eploration* or *eploitation* mode randomly:<br>\n",
    "\n",
    "**Exercise 3**:  Implement `run_one_dtep` <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; This method:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Choose an action<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Apply the action on the environement.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> return all element of the experience\n",
    "\n",
    "**Exercise 4**:  Implement `generate_target_q`<br>\n",
    "This method is used within the `train_one_step` method (which is already implemented).This method:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Generate a batch of data for training using the `experience_replay` <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Generate the targets from this batch using `generate_target_q` <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Train the model using these targets. <br>\n",
    "<br> \n",
    "The `generate_target_q` is not implemented so you have to do it!<br>\n",
    "You have to generate targets according to the formula below <br>\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a';\\theta) $$\n",
    "\n",
    "# TODO give tips for train_gameover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.prob_random = 1.0  # Probability to play random action\n",
    "        self.y = .99  # Discount factor\n",
    "        self.batch_size = 64  # How many experiences to use for each training step\n",
    "        self.prob_random_end = .01  # Ending chance of random action\n",
    "        self.prob_random_decay = .996  # Decrease decay of the prob random\n",
    "        self.max_episode = 300  # Max number of episodes you are allowes to played to train the game\n",
    "        self.expected_goal = 200  # Expected goal\n",
    "\n",
    "        self.dnn = DNN()\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "        self.metadata = [] # we will store here info score, at the end of each episode\n",
    "\n",
    "\n",
    "    def save_experience(self, experience):\n",
    "        #TODO\n",
    "        return None\n",
    "\n",
    "    def choose_action(self, state, prob_random):\n",
    "        #TODO\n",
    "        return action\n",
    "\n",
    "    def run_one_step(self, state):\n",
    "        #TODO\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def generate_target_q(self, train_state, train_action, train_reward, train_next_state, train_done):\n",
    "        #TODO\n",
    "        return target_q\n",
    "\n",
    "    def train_one_step(self):\n",
    "\n",
    "        batch_data = self.memory\n",
    "        train_state = np.array([i[0] for i in batch_data])\n",
    "        train_action = np.array([i[1] for i in batch_data])\n",
    "        train_reward = np.array([i[2] for i in batch_data])\n",
    "        train_next_state = np.array([i[3] for i in batch_data])\n",
    "        train_done = np.array([i[4] for i in batch_data])\n",
    "\n",
    "        # These lines remove useless dimension of the matrix\n",
    "        train_state = np.squeeze(train_state)\n",
    "        train_next_state = np.squeeze(train_next_state)\n",
    "\n",
    "        # Generate target Q\n",
    "        target_q = self.generate_target_q(\n",
    "            train_state=train_state,\n",
    "            train_action=train_action,\n",
    "            train_reward=train_reward,\n",
    "            train_next_state=train_next_state,\n",
    "            train_done=train_done\n",
    "        )\n",
    "\n",
    "        loss = self.dnn.model.train_on_batch(train_state, target_q)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        scores = []\n",
    "        for e in range(self.max_episode):\n",
    "            # Init New episode\n",
    "            state = self.env.reset()\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            episode_score = 0\n",
    "            while True:\n",
    "                state, action, reward, next_state, done = self.run_one_step(state)\n",
    "                self.save_experience(experience=[state, action, reward, next_state, done])\n",
    "                episode_score += reward\n",
    "                state = next_state\n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    self.train_one_step()\n",
    "                    if self.prob_random > self.prob_random_end:\n",
    "                        self.prob_random *= self.prob_random_decay\n",
    "                if done:\n",
    "                    now = datetime.now()\n",
    "                    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                    self.metadata.append([now, e, episode_score, self.prob_random])\n",
    "                    print(\n",
    "                        \"{} - episode: {}/{}, score: {:.1f} - prob_random {:.3f}\".format(dt_string, e, self.max_episode,\n",
    "                                                                                         episode_score,\n",
    "                                                                                         self.prob_random))\n",
    "                    break\n",
    "            scores.append(episode_score)\n",
    "\n",
    "            # Average score of last 100 episode\n",
    "            means_last_10_scores = np.mean(scores[-10:])\n",
    "            if means_last_10_scores == self.expected_goal:\n",
    "                print('\\n Task Completed! \\n')\n",
    "                break\n",
    "            print(\"Average over last 10 episode: {0:.2f} \\n\".format(means_last_10_scores))\n",
    "        print(\"Maximum number of episode played: %d\" % self.max_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test `save_experience`**\n",
    "* Append element to the `memory`.\n",
    "* Never save more than `batch_size` element, keep the last `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "dqn.batch_size=2\n",
    "dqn.save_experience(1)\n",
    "assert dqn.memory == [1]\n",
    "dqn.save_experience(2)\n",
    "assert dqn.memory == [1,2]\n",
    "dqn.save_experience(3)\n",
    "assert dqn.memory == [2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test `choose_action`**\n",
    "\n",
    "This test can't be considered as a real test. <br>\n",
    "Indeed, if the action are play randomly we can't expect a fixed results. \n",
    "\n",
    "However, if your function is implemented correctly these test should word most of the time:\n",
    "\n",
    "* if `prob_random` = 1 -> play randomly\n",
    "    * Over 100 play, each action should appears various time\n",
    "* If `prob_random` = 0 -> play in exploit mode\n",
    "    * The same action is choosen all the time.\n",
    "* If `prob_random` = 0.5 -> play both exploration and exploit mode randomly. \n",
    "    * All action sould be seen, but the action choosen in exploit mode is always the same and should be choosen more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "state = np.expand_dims(dqn.env.reset(), axis=0)\n",
    "# Random action if prob random is equal to one\n",
    "actions = [dqn.choose_action(state=state, prob_random=1) for _ in range(100)]\n",
    "count_action = collections.Counter(actions)\n",
    "print(count_action)\n",
    "assert count_action[0]>35\n",
    "assert count_action[1]>35\n",
    "# Best action according to model if prob_random is 0\n",
    "actions = [dqn.choose_action(state=state, prob_random=0) for _ in range(100)]\n",
    "count_action = collections.Counter(actions)\n",
    "print(count_action)\n",
    "assert(len(set(actions)))==1\n",
    "main_action = list(set(actions))[0]\n",
    "# \n",
    "actions = [dqn.choose_action(state=state, prob_random=0.5) for _ in range(100)]\n",
    "count_action = collections.Counter(actions)\n",
    "assert(len(set(actions)))==2\n",
    "print(count_action)\n",
    "assert sorted(count_action.items(), key=lambda x : x[1])[-1][0]==main_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test `run_one_step`**\n",
    "\n",
    "This method play one step of an episode.\n",
    "\n",
    "The method return all element of an experience, i.e:\n",
    " * A *state*: a vector of size (1,4)\n",
    " * An *action*: an integer\n",
    " * A *reward*: a float\n",
    " * The *nex_state*: a vector of size (1,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "state = np.expand_dims(dqn.env.reset(), axis=0)\n",
    "state, action, reward, next_state, done  = dqn.run_one_step(state)\n",
    "assert state.shape == (1, 4)\n",
    "assert type(action) is int\n",
    "assert type(reward) is float\n",
    "assert next_state.shape == (1, 4)\n",
    "assert type(done) is bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test `generate_target_q`**\n",
    "\n",
    "This method generate targets of q values.\n",
    "\n",
    "In this test we set the `batch_size`value is equal to 2. Hence the function take as an input: \n",
    "* train_state : An array of size (2,4)\n",
    "* train_action : An array of size (2,1)\n",
    "* train_reward  : An array of size (2,1)\n",
    "* train_next_state : An array of size (2,4)\n",
    "* train_done : An array of size (2,1)\n",
    "\n",
    "And return as an output an Array of size (2,2), which is a target for each input of the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "dqn.batch_size=2\n",
    "state = np.expand_dims(dqn.env.reset(), axis=0)\n",
    "target_q = dqn.generate_target_q(\n",
    "    train_state = np.vstack([state,state]),\n",
    "    train_action = [0,0],\n",
    "    train_reward = [1.0,2.0],\n",
    "    train_next_state = np.vstack([state,state]),\n",
    "    train_done = [1, 1]\n",
    ")\n",
    "\n",
    "assert target_q.shape == (2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the solution of the **DQN class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/DQN_cartpole_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model! (The training can be unstable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "dqn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're DQN reached the target goal (or not) we would like to see it playing a game!\n",
    "**Exercise** Play a game exploiting the dnn trained with deep q learning and display video of this game to check how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/play_cartpole_with_dnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below enables to display the evolution of the score of each episode play during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(list(range(len(dqn.metadata))),[x[2] for x in dqn.metadata])\n",
    "ax.set_yticks(np.arange(0,210,10))    \n",
    "ax.set_xticks(np.arange(0,175,25))    \n",
    "ax.set_title(\"Score/Lenght of episode over Iteration withou Memory Replay\", fontsize=20)\n",
    "ax.set_xlabel(\"Number of iteration\", fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "ax.set_ylabel(\"Score/Length of episode\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be lucky but it is higly possible that the training is quite unstable. \n",
    "\n",
    "As see in course, this might be to the fact that the experiences on which is trained the DNN are not i.i.d.\n",
    "Let's try again with and **Experience Replay Buffer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with Experience Replay Buffer\n",
    "\n",
    "\n",
    "The **Experience Replay Buffer** is where all the agent's experience will be stored and where *batch* will be generate from in order to train the *Q network*  \n",
    "\n",
    "**Exercise** Let'us implement an `ExperienceReplay` class which will have the following characteristics \n",
    "\n",
    "The `buffer_size` argument represent the number of element that are kept in memory (in the `buffer`). <br>\n",
    "Even if 10Milions of games have been played, the `Experience Replay` will kept only the last `buffer_size` argument in memory. <br>\n",
    "Hence at the beginning the first batch of targets will be composed of randomly played experience. And during training, the probability that batch of targets will be compose of experience playe in exploitation mode will increase.\n",
    "\n",
    "The `add` method will add elements on the `buffer `.\n",
    "\n",
    "The `sample`method will generate a sample of `size`element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        \"\"\" Data structure used to hold game experiences \"\"\"\n",
    "        # Buffer will contain [state,action,reward,next_state,done]\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experiences):\n",
    "        \"\"\" Adds list of experiences to the buffer \"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\" Returns a sample of experiences from the buffer \"\"\"\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load experience_replay.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a simple example on how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate an experience replay buffer with buffer_size 10\n",
    "experience_replay = ExperienceReplay(buffer_size=10)\n",
    "# Add list of 100 integer in the buffer\n",
    "experience_replay.add(list(range(100)))\n",
    "# Check that it keeps only the las 10 element\n",
    "print(experience_replay.buffer)\n",
    "# Randomly sample 5 element from the buffer\n",
    "sample = experience_replay.sample(5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Now that you have implemented the `ExperienceReplay` class, modify the `DQN`you implemented above, and modify it to use this class as the memory instead of a simple python list and run again the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/DQN_cartpole_memory_replay_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model! (That should be much more stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "dqn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once again let's play a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "frames = []\n",
    "num_step=0\n",
    "done=False\n",
    "while not done:\n",
    "    action=np.argmax(dqn.dnn.model.predict(np.expand_dims(state, axis=0)),axis=1)[0]\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    frames.append(env.render(mode = \"rgb_array\"))\n",
    "    state=next_state\n",
    "    num_step+=1\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And observe the evolution of the score over iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(list(range(len(dqn.metadata))),[x[2] for x in dqn.metadata])\n",
    "ax.set_yticks(np.arange(0,210,10))    \n",
    "ax.set_xticks(np.arange(0,175,25))    \n",
    "ax.set_title(\"Score/Lenght of episode over Iteration withou Memory Replay\", fontsize=20)\n",
    "ax.set_xlabel(\"Number of iteration\", fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "ax.set_ylabel(\"Score/Length of episode\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What can you say about the influence of the experience replay buffer over this training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
